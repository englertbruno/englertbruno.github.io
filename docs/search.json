[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/003_neural_tract/englert_ai_3_neural_tract.html",
    "href": "posts/003_neural_tract/englert_ai_3_neural_tract.html",
    "title": "Controlling a Vocal Tract With a Neural Network",
    "section": "",
    "text": "There is a site called Pink Trombone — a browser-based vocal tract simulation where you can drag around control points with your mouse and hear bizarrely lifelike vocal sounds. There’s no text, no phonemes. Just a simple simulation of airflow through a deformable tube that resembles the human vocal tract. As you drag the glottis slider or reshape the oral cavity, the sound changes. And if you’re patient — and precise — you can get it to say vowel-like sounds, or even whole syllables.\nThis simulator inspired a research question: could we train a neural network to automatically control this simulated vocal tract to generate intelligible speech? That is, instead of manually manipulating sliders, can a machine learn to shape the simulated vocal tract in a way that produces coherent speech? This question became the basis for an investigation into hybrid speech synthesis — a method that combines deep learning with physical models to produce speech through a controllable, differentiable physical simulation.\n\n\nUnderstanding how humans produce speech provides crucial context for modeling it computationally. Unlike conventional speech synthesis systems that directly predict audio waveforms or spectrograms, human speech is the emergent result of a physical process. It starts with airflow from the lungs, which is shaped by the glottis and further modified by the geometry of the vocal tract.\nThe glottis — the opening between the vocal folds — plays a central role. When the vocal folds are brought close and set into vibration, they produce a periodic pressure source, creating voiced sounds such as vowels. When the folds are apart and stationary, the airflow is unmodulated, leading to voiceless sounds like fricatives (e.g., “s” or “f”).\nThis sound source then propagates through the vocal tract, which includes the throat, mouth, and nasal passages. The configuration of these structures acts as a dynamic filter, amplifying or attenuating specific frequency components based on its geometry. This filtering effect, governed by the resonant properties of the vocal tract, gives rise to the acoustic qualities of different phonemes.\nFrom a modeling perspective, the key insight is that speech is not generated by manipulating acoustic properties directly but by controlling muscle movements that shape the tract. Traditional deep learning models for speech synthesis — such as WaveNet or Tacotron — bypass this by directly predicting waveform amplitudes or spectrogram features. While effective, these models do not mirror the human production mechanism and are computationally intensive due to the need to predict 44,100 audio samples per second for a high-fidelity, CD-quality audio.\n\n\n\nSimulating the vocal tract digitally isn’t a new idea. There’s a long history of simulating speech production using physics-inspired approaches, especially for articulatory synthesis. One particularly efficient approach is based on digital waveguides. To model the vocal tract physically, we rely on the theory of wave propagation in tubes. The continuous wave equation governs how sound pressure propagates through a medium:\n\\[\n\\frac{\\partial^2 p(x, t)}{\\partial t^2} = c^2 \\frac{\\partial^2 p(x, t)}{\\partial x^2}\n\\]\nwhere (p(x, t)) is the sound pressure at position (x) and time (t), and (c) is the speed of sound in air.\nThis equation can be solved numerically by discretizing both time and space. The digital waveguide model represents the solution as the sum of two discrete traveling waves — one moving rightward \\((p_r(n))\\), and one leftward \\((p_l(n))\\):\n\\[\np(x, nT) = p_r(n) + p_l(n)\n\\]\nIn Python, this simply can be represented as two n-length lists, where each floating-point list element is the magnitude of the sound pressure.\nEach segment of the vocal tract is modeled as a tube with a specific cross-sectional area. When waves travel through segments with differing cross-sectional areas (such as in the vocal tract), partial reflections occur at each junction between segments. These reflections happen due to the mismatch in acoustic impedance, which depends directly on the cross-sectional area. At each junction, the reflection coefficient \\(r\\) is defined by the relative difference of adjacent cross-sectional areas \\(A_i\\) and \\(A_{i+1}\\):\n\\[\nr = \\frac{A_i - A_{i+1}}{A_i + A_{i+1}}\n\\] In this work, a deep learning model is predicting the diameter of the cross-sections of each tract segment.\nThis reflection coefficient determines how much of the wave is reflected back and how much continues forward. We calculate the outgoing pressures \\((p^-)\\) at a junction from the incoming pressures (\\(p^+\\)) using these reflection coefficients:\n\\[\n\\begin{aligned}\np_i^- &= p_{i+1}^+ + r(p_i^+ - p_{i+1}^+) \\\\\np_{i+1}^- &= p_i^+ + r(p_i^+ - p_{i+1}^+)\n\\end{aligned}\n\\] With this one equation, we can calculate the next new sound pressures as they propagate through the digital waveguide. This digital waveguide, as said previously, is simply represented as two lists in Python. We will use this equation as is in our code to do the simulation.\nThrough these discretized equations, the digital waveguide captures the physics of wave reflections and transmissions, allowing a somewhat realistic simulation of a vocal tract. This is only glimpse into how digital waveguide work, for a more complete explanation please read Jack Mullen’s PhD thesis: Physical Modelling of the Vocal Tract with the 2D Digital Waveguide Mesh"
  },
  {
    "objectID": "posts/003_neural_tract/englert_ai_3_neural_tract.html#how-humans-speak",
    "href": "posts/003_neural_tract/englert_ai_3_neural_tract.html#how-humans-speak",
    "title": "Controlling a Vocal Tract With a Neural Network",
    "section": "",
    "text": "Understanding how humans produce speech provides crucial context for modeling it computationally. Unlike conventional speech synthesis systems that directly predict audio waveforms or spectrograms, human speech is the emergent result of a physical process. It starts with airflow from the lungs, which is shaped by the glottis and further modified by the geometry of the vocal tract.\nThe glottis — the opening between the vocal folds — plays a central role. When the vocal folds are brought close and set into vibration, they produce a periodic pressure source, creating voiced sounds such as vowels. When the folds are apart and stationary, the airflow is unmodulated, leading to voiceless sounds like fricatives (e.g., “s” or “f”).\nThis sound source then propagates through the vocal tract, which includes the throat, mouth, and nasal passages. The configuration of these structures acts as a dynamic filter, amplifying or attenuating specific frequency components based on its geometry. This filtering effect, governed by the resonant properties of the vocal tract, gives rise to the acoustic qualities of different phonemes.\nFrom a modeling perspective, the key insight is that speech is not generated by manipulating acoustic properties directly but by controlling muscle movements that shape the tract. Traditional deep learning models for speech synthesis — such as WaveNet or Tacotron — bypass this by directly predicting waveform amplitudes or spectrogram features. While effective, these models do not mirror the human production mechanism and are computationally intensive due to the need to predict 44,100 audio samples per second for a high-fidelity, CD-quality audio."
  },
  {
    "objectID": "posts/003_neural_tract/englert_ai_3_neural_tract.html#simulating-a-vocal-tract-the-digital-waveguide",
    "href": "posts/003_neural_tract/englert_ai_3_neural_tract.html#simulating-a-vocal-tract-the-digital-waveguide",
    "title": "Controlling a Vocal Tract With a Neural Network",
    "section": "",
    "text": "Simulating the vocal tract digitally isn’t a new idea. There’s a long history of simulating speech production using physics-inspired approaches, especially for articulatory synthesis. One particularly efficient approach is based on digital waveguides. To model the vocal tract physically, we rely on the theory of wave propagation in tubes. The continuous wave equation governs how sound pressure propagates through a medium:\n\\[\n\\frac{\\partial^2 p(x, t)}{\\partial t^2} = c^2 \\frac{\\partial^2 p(x, t)}{\\partial x^2}\n\\]\nwhere (p(x, t)) is the sound pressure at position (x) and time (t), and (c) is the speed of sound in air.\nThis equation can be solved numerically by discretizing both time and space. The digital waveguide model represents the solution as the sum of two discrete traveling waves — one moving rightward \\((p_r(n))\\), and one leftward \\((p_l(n))\\):\n\\[\np(x, nT) = p_r(n) + p_l(n)\n\\]\nIn Python, this simply can be represented as two n-length lists, where each floating-point list element is the magnitude of the sound pressure.\nEach segment of the vocal tract is modeled as a tube with a specific cross-sectional area. When waves travel through segments with differing cross-sectional areas (such as in the vocal tract), partial reflections occur at each junction between segments. These reflections happen due to the mismatch in acoustic impedance, which depends directly on the cross-sectional area. At each junction, the reflection coefficient \\(r\\) is defined by the relative difference of adjacent cross-sectional areas \\(A_i\\) and \\(A_{i+1}\\):\n\\[\nr = \\frac{A_i - A_{i+1}}{A_i + A_{i+1}}\n\\] In this work, a deep learning model is predicting the diameter of the cross-sections of each tract segment.\nThis reflection coefficient determines how much of the wave is reflected back and how much continues forward. We calculate the outgoing pressures \\((p^-)\\) at a junction from the incoming pressures (\\(p^+\\)) using these reflection coefficients:\n\\[\n\\begin{aligned}\np_i^- &= p_{i+1}^+ + r(p_i^+ - p_{i+1}^+) \\\\\np_{i+1}^- &= p_i^+ + r(p_i^+ - p_{i+1}^+)\n\\end{aligned}\n\\] With this one equation, we can calculate the next new sound pressures as they propagate through the digital waveguide. This digital waveguide, as said previously, is simply represented as two lists in Python. We will use this equation as is in our code to do the simulation.\nThrough these discretized equations, the digital waveguide captures the physics of wave reflections and transmissions, allowing a somewhat realistic simulation of a vocal tract. This is only glimpse into how digital waveguide work, for a more complete explanation please read Jack Mullen’s PhD thesis: Physical Modelling of the Vocal Tract with the 2D Digital Waveguide Mesh"
  },
  {
    "objectID": "posts/003_neural_tract/englert_ai_3_neural_tract.html#glottal-source",
    "href": "posts/003_neural_tract/englert_ai_3_neural_tract.html#glottal-source",
    "title": "Controlling a Vocal Tract With a Neural Network",
    "section": "Glottal Source",
    "text": "Glottal Source\n\n\n\n\n\n\n\nFigure 2: The consonants mostly occupy the 6-20kHz range as highlighted with the red rectangles.\n\n\n\nA purely digital waveguide based vocal tract alone isn’t sufficient to simulate realistic speech because the vocal folds (glottis) play a crucial role in generating the raw sound and adding energy into the system. The glottis itself is particularly important for voiced sounds like vowels. However, full glottal modeling is complex and would typically require a dedicated sub-network or parametric model (e.g. glottal flow models). For the sake of keeping things simple for this blog post, we introduce a gross oversimplification: the glottal source primarily contributes to low-frequency energy (under ~6kHz), while higher-frequency components are produced by the tract.\nThis is based on the assumption that voiced sounds (vowels) primarily occupy lower frequencies, which are heavily influenced by glottal vibration, while voiceless sounds (like ‘s’ or ‘f’) often appear as white noise in higher frequencies (in fact, the glottis itself is usually not able to vibrate more that ~1100Hz source). This assumption allows us to bypass complex deep learning methods (like WaveGlow a neural network-based vocoder from 2018) for glottal prediction, focusing instead on predicting only the higher frequencies with the digital vocal tract.\nThus, we extract the 0–6kHz band from the original audio to serve as a surrogate glottal source and use white noise as a high-frequency excitation input to the digital waveguide."
  },
  {
    "objectID": "posts/003_neural_tract/englert_ai_3_neural_tract.html#hybrid-speech-synthesis",
    "href": "posts/003_neural_tract/englert_ai_3_neural_tract.html#hybrid-speech-synthesis",
    "title": "Controlling a Vocal Tract With a Neural Network",
    "section": "Hybrid Speech Synthesis",
    "text": "Hybrid Speech Synthesis\nWe use a neural network to predict the tract diameters. The input to this neural network is the mel-spectrogram, from which it predicts the tract diameters. These diameters directly influence the reflection coefficients and thus determine the resonance and filtering properties of the simulated vocal tract.\nTo be able to output pressure, we need to input pressure (or, more precisely, energy) to the simulated tract. To do so, we feed white noise into the digital waveguide. This white noise serves as a neutral input (same way the lung puts pressure/energy into the vocal tract) that is shaped by the predicted vocal tract diameters.\nThe final synthesized audio is created by adding together the original 0-6kHz audio (representing the glottal output) and the high-frequency audio generated by the digital waveguide simulation."
  },
  {
    "objectID": "posts/003_neural_tract/englert_ai_3_neural_tract.html#training-loss",
    "href": "posts/003_neural_tract/englert_ai_3_neural_tract.html#training-loss",
    "title": "Controlling a Vocal Tract With a Neural Network",
    "section": "Training Loss",
    "text": "Training Loss\nBackpropagation is performed through both the neural network and the digital waveguide, enabling end-to-end learning of a system that physically models speech synthesis. The training loss uses a Mean Squared Error (MSE) loss between the spectrograms of the original and generated audio, alongside an L1 loss for the raw audio wave values:\n\\[\n\\mathcal{L} = \\text{MSE}(\\text{Spec}_{original}, \\text{Spec}_{generated}) + \\lambda \\cdot \\text{L1}(\\text{Audio}_{original}, \\text{Audio}_{generated})\n\\]\nwhere \\(\\text{Spec}_{\\text{orig}}\\) and \\(\\text{Spec}_{\\text{gen}}\\) are the spectrograms of the original and generated audio respectively, and \\(\\text{Audio}_{\\text{orig}}\\), \\(\\text{Audio}_{\\text{gen}}\\) are the raw waveforms. The hyperparameter \\(\\lambda\\) balances the two loss components."
  },
  {
    "objectID": "posts/003_neural_tract/englert_ai_3_neural_tract.html#parallelizing-the-digital-waveguide",
    "href": "posts/003_neural_tract/englert_ai_3_neural_tract.html#parallelizing-the-digital-waveguide",
    "title": "Controlling a Vocal Tract With a Neural Network",
    "section": "Parallelizing the Digital Waveguide",
    "text": "Parallelizing the Digital Waveguide\n\n\n\n\n\n\n\nFigure 3: For the autoregressive waveguide, “four” sound pressures enter one by one, while for the parallelized waveguide, all the sound pressures enter at once into “four” different waveguides.\n\n\n\nA major challenge in using a traditional digital waveguides for high-fidelity audio synthesis is that they compute wave propagation sequentially, timestep by timestep. This means that generating one second of audio at a sampling rate of 44.1kHz, we would need to execute a “for loop” of 44,100 sequential steps. This makes digital waveguides computationally slow on parallel hardware such as GPUs. The issue is exacerbated that we would have to do backpropagation through 44,100 steps, which causes out of memory issues and the training would not be even possible.\nTo address this, we exploit the linearity of the wave equation: each individual input to the vocal tract simulation can be treated independently. Because wave propagation in air interfere additively, the total response of the system to a sequence of inputs can be computed as the sum of the system’s response to each input individually. In our case, this means that the total output from the vocal tract can be expressed as the sum of outputs resulting from each individual input sample.\nThus, we can parallelize the simulation by treating each input excitation (e.g., an impulse at a given time) as an independent event. Rather than simulating the tract state step-by-step over time, we simulate how each input sample propagates through the vocal tract independently. Since the system is linear, we can simultaneously compute these responses, and after running these independent simulations, the total output can simply be obtained by adding up the contributions from each input sample. This reframing transforms the process from a time-ordered simulation into a parallel computation over independent events.\nDue to energy losses from reflections and boundary conditions (e.g., at the mouth or nasal cavity), the influence of each input to the vocal tract decays rapidly. In practice, this means that after a few dozens of simulation steps, the amplitude of an input is so small that it is no longer significant to the output, and thus we can ignore it. We can use this to our advantage and limit the number of simulation steps. By limiting each simulation to a small, finite number of steps, roughly corresponding to the length of the vocal tract (plus a few additional steps), we ensure the computation remains efficient.\nIn short, parallelization is achieved by independently computing wave propagation for all input samples in parallel and then summing their contributions. This parallelizing approach makes a physical digital wave simulation feasible on modern GPUs. On top of this, all previously described steps are differentiable, thus making an end-to-end training possible."
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "",
    "text": "With some practice, it’s possible to balance a broom upright in our hand, relying largely on intuition rather than accurate measurements. Yet, the dynamics behind this seemingly simple act are more intricate than they appear. From an engineering perspective, this balancing act is analogous to the inverted pendulum, a classic problem in control theory.\nThe inverted pendulum is a system consisting of a tall rod, like our broom, but this time, it’s attached to a horizontally moving cart. The objective? Keep the rod upright, compensating for any disturbances that might knock it off balance. This control problem is complex due to the numerous degrees of freedom involved, but still straightforward enough to understand and simulate.\nWhile mathematical solutions exist to keep the pendulum vertically upright, they require precise knowledge of the system’s parameters. Yet when we balance a broom, humans are clearly not relying on a mathematical model that depends on highly accurate measurements of angles and velocities. This suggests that for the inverted pendulum problem, it’s possible to design a successful controller without depending on precise measurements.\nWhat if we wanted to introduce an element of that human intuition into our control systems? Enter fuzzy logic—a method that resembles human reasoning by working with concepts that aren’t strictly true or false but lie somewhere in between. For example, instead of processing exact numerical values, a fuzzy logic system might consider values like “almost upright” or “slightly tilted.” This approach offers flexibility, especially when precise data is unavailable or when a system needs to adjust to unforeseen challenges.\nIn the early 2000s, the number and variety of fuzzy logic applications increased significantly. They ranged from consumer products such as cameras, camcorders, washing machines, and microwave ovens to industrial process control, medical instrumentation, decision-support systems, and portfolio selection. Unlike standard logic, where variables can only take two values—True or False—fuzzy logic describes things in a vaguer form. Its variables can range between 0 and 1, characterizing a variable’s membership to a particular value. For instance, describing weather as “0.72 sunny and 0.18 cloudy” provides more information than simply stating “the weather today is sunny but not cloudy.” With fuzzy variables, one can construct operations known as fuzzy rules. These rules resemble human thinking and are structured as “If … then…” However, translating these rules into a soft mathematical form suitable for machine processing is essential.\n\n\n\nFigure 1: Forces in the free-body diagram\n\n\nReturning to our inverted pendulum on a cart problem, this system can be seen in Figure 1. Intuitively, the control force \\(F\\) should be determined by the magnitudes of the input variables \\(\\phi\\) and \\(\\dot{\\phi}\\), which measure the angle of the pendulum from the upright position and its angular velocity, respectively. Using fuzzy logic, the relationship between these variables becomes linguistic, a much weaker form than exact measurements. Leveraging these linguistic variables, we can establish rules that determine the control \\(F\\) using common sense knowledge. These rules can look like such as “If \\(\\phi\\) is very small, then \\(F\\) should be small,” or “If the pendulum is balanced, then hold very still, meaning do not apply any force.”"
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#linguistic-variables",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#linguistic-variables",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "Linguistic Variables",
    "text": "Linguistic Variables\nLinguistic variables are either the input or output variables of the system. Their values are words or sentences from natural language, not numerical values. Typically, a linguistic variable is broken down into a set of linguistic terms. For example, in the context of an inverted pendulum, let the angle \\(\\phi\\) be the linguistic variable symbolizing the angle formed with the upper vertical position. To qualify the angle, we naturally use linguistic terms such as “slightly slanted” and “falling over”. Using this intuition, we can define a mapping functions, such as \\(\\phi(t)=\\) {negative big (NB) angle, negative medium (NM) angle, negative small (NS) angle, positive small (PS) angle, positivev medium (PM) angle, and positive big (PB) angle}. This can be the set of decomposition for the linguistic variable angle. Each member of this decomposition is called a linguistic term and can cover a portion of the overall values of the angle."
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#membership-functions",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#membership-functions",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "Membership Functions",
    "text": "Membership Functions\nMembership functions are used in the fuzzification and defuzzification steps of the fuzzy logic system, to map the non-fuzzy input values to fuzzy linguistic terms and vice versa. A membership function quantifies a linguistic term. For instance, Figure [fig:Membership-funtions-of-angle] shows the membership functions for the linguistic terms of the angle variable. An important characteristic of fuzzy logic is that a numerical value does not have to be fuzzified using only one membership function. In other words, a value can belong to multiple sets at the same time. For example, according to Figure [fig:Membership-funtions-of-angle], an angle value can be considered as “negative small” and “negative medium” at the same time, with different degrees of memberships."
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#fuzzy-rules",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#fuzzy-rules",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "Fuzzy Rules",
    "text": "Fuzzy Rules\nWithin a fuzzy logic system, a rule base is constructed to control the output variable. A fuzzy rule is a simple IF-THEN rule with a condition and a conclusion."
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#fuzzy-set-operations",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#fuzzy-set-operations",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "Fuzzy Set Operations",
    "text": "Fuzzy Set Operations\nThe evaluations of the fuzzy rules and the combination of the results of the individual rules are performed using fuzzy set operations. The operations on fuzzy sets are different from the operations on non-fuzzy sets. Let \\(\\mu_A\\) and \\(\\mu_B\\) represent the membership functions for fuzzy sets \\(A\\) and \\(B\\). Unlike in conventional Boolean algebra we can have different definitions for the logic operator. The table below lists potential fuzzy operations for OR and AND operators:\n\n\n\n\n\n\n\n\n\n\nOR (Union)\n\nAND (Intersection)\n\n\n\n\nMAX\n\\(Max\\{\\mu_A(x), \\mu_B(x)\\}\\)\nMIN\n\\(Min\\{\\mu_A(x), \\mu_B(x)\\}\\)\n\n\nASUM\n\\(\\mu_A(x) + \\mu_B(x) - \\mu_A(x) \\mu_B(x)\\)\nPROD\n\\(\\mu_A(x) \\mu_B(x)\\)\n\n\nBSUM\n\\(Min\\{1, \\mu_A(x) + \\mu_B(x)\\}\\)\nBDIF\n\\(Max\\{0, \\mu_A(x) + \\mu_B(x) - 1\\}\\)\n\n\n\nAfter evaluating the result of each rule, these results need to be merged to obtain a final result. This process is called inference. There are different methods that can combine the outcomes of individual rules. The table below contains possible accumulation methods that are used to combine the results of individual rules. The MAX operator is typically used for accumulation.\n\n\n\n\n\n\n\nOperation\nFormula\n\n\n\n\nMaximum\n\\(Max\\{\\mu_A(x),\\mu_B(x)\\}\\)\n\n\nBounded sum\n\\(Min\\{1, \\mu_A(x)+\\mu_B(x)\\}\\)\n\n\nNormalized sum\n\\(\\frac{\\mu_A(x)+\\mu_B(x)}{Max\\{1,Max\\{\\mu_A(x^{'}),\\mu_B(x^{'})\\}\\}}\\)"
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#defuzzification",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#defuzzification",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "Defuzzification",
    "text": "Defuzzification\nThe final step is defuzzification. Following the inference step, we are left with a fuzzy value. This value must be transformed aka defuzzified, to yield our control output. Defuzzification is carried out based on the membership function of the output variable. The result of the inference step is illustrated on Figure 3, where the shaded areas collectively represent the fuzzy outcome. Our objective is to extract a definitive value from this fuzzy representation. This is denoted by a dot in the figure. The defuzzification process is depicted in Figure 3. Several algorithms exist for defuzzification, where the most commonly used algorithms are listed in this table:\n\n\n\n\n\n\n\nOperation\nFormula\n\n\n\n\nCenter of Gravity\n\\(\\frac{\\int_{min}^{max}u\\mu(u) du}{\\int_{min}^{max}\\mu(u)du}\\)\n\n\nCenter of Gravity for Singletons\n\\(\\frac{\\sum_{i=1}^{p}[\\mu_iu_i]}{\\sum_{i=1}^{p}[\\mu_i]}\\)\n\n\n\n\n\n\nFigure 3: Defuzzification"
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#input-parameters-and-fuzzification",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#input-parameters-and-fuzzification",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "1. Input Parameters and Fuzzification",
    "text": "1. Input Parameters and Fuzzification\nIn the fuzzy control system designed for this application, the primary inputs are the angle \\(\\phi\\) and angular speed \\(\\dot\\phi\\). These values represent the current state of the system and serve as the basis for our fuzzy logic evaluations.\nTo make these numerical inputs compatible with the fuzzy control system, they need to be fuzzified. Fuzzification is the process of assigning a degree of membership to each value in a set. For our system, the membership functions used are Gaussian in nature and are defined as: \\(f(x)=e^\\frac{-(x-a)^2}{2\\sigma^2}\\)\nThe fuzzified values are then categorized into six linguistic variables, represented as: - NB (Negative Big) - NM (Negative Medium) - NS (Negative Small) - PS (Positive Small) - PM (Positive Medium) - PB (Positive Big)\nOne might wonder, that if we have access to accurate measurements, which is the case in this simulation since we are running a simulation, why would we intentionally get rid of accurate measurements? The reason is that this is just a toy example for fuzzy control logic, and even though we have accurate measurements, we are not going to take advantage of this fact.\n\n\n\nFigure 4: Membership functions of angle\n\n\n\n\n\nFigure 5: Membership functions of angular speed"
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#rule-base-construction",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#rule-base-construction",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "2. Rule Base Construction",
    "text": "2. Rule Base Construction\nUsing the these linguistic variables, we construct a comprehensive rule base, as outlined in the table below:\n\n\n\n\\(\\phi\\textbackslash\\dot{\\phi}\\)\nNB\nNM\nNS\nPS\nPM\nPB\n\n\n\n\nNB\nNB\nNB\nNB\nNM\nNS\nPS\n\n\nNM\nNB\nNB\nNM\nNS\nPS\nPS\n\n\nNS\nNB\nNM\nNS\nPS\nPS\nPM\n\n\nPS\nNM\nNS\nNS\nPS\nPM\nPB\n\n\nPM\nNS\nNS\nPS\nPM\nPB\nPB\n\n\nPB\nNS\nPS\nPM\nPB\nPB\nPB\n\n\n\nThis rule table should be read the following way:\n• \\(\\textbf{\\textit{IF angle is NB and angular speed is NB then the control force is NB.}}\\)\n• \\(\\textbf{\\textit{IF angle is NB and angular speed is NM then the control force is NB.}}\\)\n\\(\\vdots\\)\n• \\(\\textbf{\\textit{IF angle is PB and angular speed is PB then the control force is PB.}}\\)"
  },
  {
    "objectID": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#defuzzification-1",
    "href": "posts/002_fuzzy_control/englert_ai_2_fuzzy_control.html#defuzzification-1",
    "title": "Balancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control",
    "section": "3. Defuzzification",
    "text": "3. Defuzzification\nAfter establishing the membership values for each control force label (NB, NM, NS, PS, PM, PB), we proceed to the defuzzification step. This is crucial as the fuzzy logic system produces outputs in the form of a range of values, which needs to be translated into a distinct scalar output to implement in real-world systems.\nFor this purpose, the Centre of Gravity for Singletons operation is employed. This algorithm computes a crisp value, representing the control force to be applied to the cart. In simpler terms, based on the fuzzy outputs and the associated membership values, it calculates a single, unambiguous control force that best represents the desired action."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "🤖 Welcome!",
    "section": "",
    "text": "Why Do Vision Transformers Need So Much Data and How to (Partially) Fix It?\n\n\n\n\n\n\ndeep\n\n\n\n\n\n\n\n\n\nJul 9, 2025\n\n\nBruno Englert\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nControlling a Vocal Tract With a Neural Network\n\n\n\n\n\n\ndeep\n\n\n\n\n\n\n\n\n\nApr 20, 2025\n\n\nBruno Englert\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nBalancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control\n\n\n\n\n\n\ncontrol\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nBruno Englert\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n2D Physics of The Inverted Pendulum on a Cart\n\n\n\n\n\n\nsim\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nBruno Englert\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html",
    "href": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html",
    "title": "Why Do Vision Transformers Need So Much Data and How to (Partially) Fix It?",
    "section": "",
    "text": "There’s a persistent belief in the machine learning community that Vision Transformers (ViTs) require massive datasets to perform well. Statements like “ViTs need millions of images” or “they don’t work without ImageNet-21k” are commonly accepted truths. But rarely do we pause to ask why are ViTs so data-hungry?\nIn this post, we argue that the answer lies in the very core of what makes ViTs different: the attention mechanism. We’ll explore why attention creates problems early in training, how it leads to noisy gradients and unstable representations — and describe one way to address the instability. This method does not fix all the challenges; notably, it does not solve the issue of learning attention patterns that generalize well from limited data. But it does reduce training instability, especially when batch sizes are small."
  },
  {
    "objectID": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#vits-and-global-attention",
    "href": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#vits-and-global-attention",
    "title": "Why Do Vision Transformers Need So Much Data and How to (Partially) Fix It?",
    "section": "ViTs and Global Attention",
    "text": "ViTs and Global Attention\nTransformers were originally designed for text, where each token (word or subword) attending to every other token makes sense — language is naturally long-range and sequential. But in images, the locality of information matters. Neighboring pixels and patches are far more likely to be related than distant ones.\nViTs ignore this. Every patch attends to every other patch from the very start of training. That means if attention weights are random (which they are at initialization), the representation at each patch is a jumbled mix of the entire image. Early layers smear information across the whole input."
  },
  {
    "objectID": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#gradient-noise-and-instability",
    "href": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#gradient-noise-and-instability",
    "title": "Why Do Vision Transformers Need So Much Data and How to (Partially) Fix It?",
    "section": "Gradient Noise and Instability",
    "text": "Gradient Noise and Instability\nSince each image in a batch has a different attention pattern, the resulting gradients are highly inconsistent — even within a single batch. This leads to high gradient noise and thus poor learning.\nTwo key problems arise from this:\n\nInformation mixing: Early attention layers destroy spatial locality. At the very end of the model’s output layer, a patch’s embedding contains little meaningful information about the patch itself.\nHigh gradient noise: Because attention is input-dependent, every image induces a unique mixing pattern, causing gradients to fluctuate wildly across the batch.\n\nTogether, these effects make training ViTs difficult when the batch size is small."
  },
  {
    "objectID": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#why-cnns-dont-suffer-from-this",
    "href": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#why-cnns-dont-suffer-from-this",
    "title": "Why Do Vision Transformers Need So Much Data and How to (Partially) Fix It?",
    "section": "Why CNNs Don’t Suffer from This",
    "text": "Why CNNs Don’t Suffer from This\nCNNs by design have inductive bias for locality and translation invariance. Filters are shared across space, and only small neighborhoods are considered at each layer. This makes them stable and thus doesn’t require a larger batch size to lower the gradient noise. ViTs, on the other hand, start with no spatial bias. They must learn to be local and how to use attention."
  },
  {
    "objectID": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#what-is-soft-spatial-attention-bias-annealing",
    "href": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#what-is-soft-spatial-attention-bias-annealing",
    "title": "Why Do Vision Transformers Need So Much Data and How to (Partially) Fix It?",
    "section": "What Is Soft Spatial Attention Bias Annealing?",
    "text": "What Is Soft Spatial Attention Bias Annealing?\nOne way to improve early ViT training when using a small batch size, is by adding a soft spatial attention bias that makes early attention focus on nearby patches.\nThis bias is computed using pairwise Euclidean distances between patch positions. The further away two patches are, the less they attend to each other — controlled by a temperature parameter that is slowly increased during training. Low temperature means attention is almost identity-like (each patch attends to itself); high temperature restores full global attention.\nThis temperature change during the training can be interpreted as a form of attention annealing — gradually releasing the spatial constraint as the model becomes more stable.\n\n\n\n\n\n\n\nFigure 1: Visualization of soft spatial attention bias annealing. Each grid cell represent the attention bias for one patch token. Initially, attention is turned off. As training progresses, it is strongly localized to neighboring patches. Eventually, as the bias temperature increases, it gradually restores the vanilla global attention."
  },
  {
    "objectID": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#code-snippet",
    "href": "posts/004_vit_attention_mask/englert_ai_4_vit_attention_mask.html#code-snippet",
    "title": "Why Do Vision Transformers Need So Much Data and How to (Partially) Fix It?",
    "section": "Code Snippet",
    "text": "Code Snippet\nHere’s the code to generate the attention bias:\ndef get_soft_spatial_attention_bias(temperature: float, img_size: int, patch_size: int) -&gt; torch.Tensor:\n    \"\"\"\n    Create a soft spatial attention bias based on Euclidean distance between image patch positions.\n    \"\"\"\n    # Create a [H*W, 2] grid of 2D positions\n    coords = torch.stack(torch.meshgrid(\n        torch.arange(img_size // patch_size),\n        torch.arange(img_size // patch_size),\n        indexing='xy'), dim=-1\n    )\n    coords = coords.reshape(-1, 2)  # [num_tokens, 2]\n\n    # Compute pairwise squared Euclidean distances: [num_tokens, num_tokens]\n    dists = torch.cdist(coords.float(), coords.float(), p=2)\n\n    # Apply temperature\n    bias = torch.softmax(-dists / temperature, dim=-1)\n    bias = torch.log(bias)\n\n    # Reshape to [1, 1, num_tokens, num_tokens] for use in attention\n    bias = bias.unsqueeze(0).unsqueeze(0)\n\n    return bias\nWe add this bias to the attention logits. It acts like a soft mask that encourages each token to attend mostly to nearby tokens — very much like a convolution. As training progresses, we increase the temperature to flatten the bias, eventually restoring global attention."
  },
  {
    "objectID": "posts/001_inverted_pendulum/englert_ai_1_cart_physics.html",
    "href": "posts/001_inverted_pendulum/englert_ai_1_cart_physics.html",
    "title": "2D Physics of The Inverted Pendulum on a Cart",
    "section": "",
    "text": "The system we are going to describe consists of an inverted pendulum mounted on a motorized cart. This is very similar when you want to try keep the broom upwards on the tip of your finger. But why is it challenging?\nThe problem is inherently unstable. Like our broomstick, once it starts to move away from its equilibrium position (straight up), gravity works to accelerate its fall. The system is also non-linear. The relation between inputs (force we exert on the broomstick) and outputs (the broomstick’s angle) isn’t directly proportional. That makes predicting and controlling its behavior a beautiful puzzle. This system is a classic example of a nonlinear and inherently unstable setup, but it’s also simple enough to understand and simulate.\nTo make our life easier, let’s consider a simplified two-dimensional case. This system consists of two parts: the cart and the pendulum. The cart can only move horizontally, and the pendulum can freely swing back and forth. We can control the system by applying a horizontal force, \\(F\\), on the cart. We get feedback from the system in the form of three observations: the angle of the pendulum, the angular velocity and its angular acceleration. Using the equations of motions can help us understand how to control and balance such a system.\n\n\n\nForces on the cart. Red denotes forces exerted on the cart, while green forces denote forces exerted on the pendulum. ‘x’ is the position of the cart."
  },
  {
    "objectID": "posts/001_inverted_pendulum/englert_ai_1_cart_physics.html#carts-motion",
    "href": "posts/001_inverted_pendulum/englert_ai_1_cart_physics.html#carts-motion",
    "title": "2D Physics of The Inverted Pendulum on a Cart",
    "section": "Cart’s Motion",
    "text": "Cart’s Motion\nLet’s start with the motion of the cart. When we push the cart, it experiences several forces: its own inertia due to its mass “M,” a damping factor “b” related to friction, and the force “N” coming from the pendulum.\n\\[\\begin{equation}\n    M\\ddot{x}+b\\dot{x}+N = F\n\\end{equation}\\]\n\n\\(M\\) is the mass of the cart.\n\\(b\\) friction coefficient.\n\\(\\dot{x}\\) is the velocity of the cart.\n\\(\\ddot{x}\\) is the acceleration of the cart.\n\\(N\\) is the reaction force from the pendulum.\n\\(F\\) is the force we apply to the cart.\n\nThe term \\(M\\ddot{x}\\) represents the force due to the cart’s acceleration. Essentially, Newton’s second law tells us that the force exerted on an object is directly proportional to its mass and acceleration.\nThe next term, \\(b\\dot{x}\\), accounts for the force opposing the cart’s movement, primarily due to friction. The friction coefficient, \\(b\\), quantifies the amount of resistive force the cart faces, which increases with the cart’s velocity \\(\\dot{x}\\). So, the faster the cart is moving, the greater the friction force it encounters.\nLastly, \\(N\\) is the reaction force exerted by the pendulum on the cart. As the pendulum swings, it applies a force on the cart, which the cart must counteract to maintain balance. We don’t know \\(N\\) yet, that’s why we just noted it with an arbitrarily chosen letter. In the following section we are determining the value \\(N\\) takes.\nNote that we only summed the forces on the horizontal axis. You can also sum the forces in the vertical direction for the cart, but since the cart is unable to move vertically, this is unnecessary."
  },
  {
    "objectID": "posts/001_inverted_pendulum/englert_ai_1_cart_physics.html#pendulums-motion",
    "href": "posts/001_inverted_pendulum/englert_ai_1_cart_physics.html#pendulums-motion",
    "title": "2D Physics of The Inverted Pendulum on a Cart",
    "section": "Pendulum’s Motion",
    "text": "Pendulum’s Motion\nWhen discussing the motion of the pendulum, it’s crucial to consider both the horizontal and vertical forces acting on it. This is because the pendulum, unlike the cart, can move in both directions due to its rotational nature.\n\nHorizontal\nThe horizontal forces acting on the pendulum are captured by the equation:\n\\[\\begin{equation}\nN = m\\ddot{x}+ml\\ddot{\\theta}\\cos\\theta-ml\\dot{\\theta}^2\\sin\\theta\n\\end{equation}\\]\n\n\\(m\\) is the mass of the pendulum.\n\\(l\\) is the length of the pendulum.\n\\(\\theta\\) is the angle of the pendulum.\n\\(\\dot{\\theta}\\) is the angular velocity\n\\(\\ddot{\\theta}\\) is the angular acceleration.\n\\(N\\) is the pendulum’s reaction force.\n\nThis equation essentially describes how the pendulum’s motion is influenced by the cart’s movement and its own inertia. The term \\(m\\ddot{x}\\) is the force exerted on the pendulum due to the acceleration of the cart. The term \\(ml\\ddot{\\theta}\\cos\\theta\\) describes the force on the pendulum due to its own angular acceleration, while −\\(ml\\dot{\\theta}^2\\sin\\theta\\) accounts for the centrifugal force acting on the pendulum as it swings.\nNow, we can substitute this back in to the cart’s equation, which gives us: \\[\\begin{equation}\n    (M+m)\\ddot{x}+b\\dot{x}+ml\\ddot{\\theta}\\cos\\theta-ml\\dot{\\theta}^2\\sin\\theta=F\n\\label{eq:horizontal}  \\tag{1}\n\\end{equation}\\]\n\n\nVertical\nSo far we only dealt with the horizontal forces. To get the vertical forces for this system, we sum the forces perpendicular to the pendulum:\n\\[\\begin{equation}\nP\\sin\\theta+N\\cos\\theta-mg\\sin\\theta=ml\\ddot{\\theta}+m\\ddot{x}\\cos\\theta\n\\end{equation}\\]\n\n\\(P\\) is the vertical component the cart is pushing back on the pendulum.\n\\(g\\) is the acceleration due to gravity.\n\nIn this equation, \\(P\\) is the vertical component of the reaction force with which the cart pushes back on the pendulum. The gravitational force acting on the pendulum is represented by \\(mg\\sin\\theta\\), which tries to pull the pendulum downward. The right side of the equation relates to the forces due to the pendulum’s angular acceleration and the cart’s linear acceleration.\nWe need to get rid of \\(P\\) and \\(N\\). To do that we sum the moments about the centroid of the pendulum to get the following equation: \\[\\begin{equation}\n-Pl\\sin\\theta-Nl\\cos\\theta=I\\ddot{\\theta}\n\\end{equation}\\]\n\n\\(I\\) is the mass moment of inertia of the pendulum.\n\nCombining these last two expressions, we get the vertical forces of the system: \\[\\begin{equation}\n(I+ml^2)\\ddot{\\theta}+mgl\\sin\\theta=-ml\\ddot{x}\\cos\\theta\n\\label{eq:vertical}  \\tag{2}\n\\end{equation}\\]\nIn our case \\(I=m(\\frac{l}{2})^2\\), (you can look up the moment of inertia for different shaped objects)."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nJul 9, 2025\n\n\nWhy Do Vision Transformers Need So Much Data and How to (Partially) Fix It?\n\n\n\n\nApr 20, 2025\n\n\nControlling a Vocal Tract With a Neural Network\n\n\n\n\nJul 29, 2023\n\n\nBalancing the Inverted Pendulum: An Introduction to Fuzzy Logic Control\n\n\n\n\nJul 26, 2023\n\n\n2D Physics of The Inverted Pendulum on a Cart\n\n\n\n\n\nNo matching items"
  }
]